# TOKIO Automated Benchmark Collection - TOKIO-ABC(tm)

## Introduction

TODO

## Test Matrix (NERSC)

Benchmark | Nodes | Procs | GiB/proc  | I/O Motif
----------|-------|-------|-----------|------------------------------------
IOR       |  96   | 1536  | 4.0       | POSIX; file per process; write+read
IOR       |  96   | 1536  | 4.0       | MPI-IO; shared file; write+read
HACC-IO   |  96   | 1536  | 0.742     | GLEAN; file per process; write
HACC-IO   |  96   | 1536  | 0.742     | GLEAN; file per process; read
VPIC-IO   |  96   | 1536  | 1.0       | pHDF5; shared file; write
BDCATS-IO |  96   | 1536  | 1.0       | pHDF5; shared file; read

## Benchmark Descriptions

### IOR

The amount of data and the pattern by which it is accessed by the IOR benchmark
code is primarily a product of three configuration parameters:

- `transferSize` (or the -t CLI option)
- `blockSize` (or the -b CLI option)
- `segmentCount` (or the -s CLI option)
- `numTasks` (or the -N CLI option, or whatever is passed to mpirun/srun/aprun)

As an example, the absolute performance of the Lustre file system on NERSC's
Cori was achieved using the following job:

    srun -N 960 -n 3840 --ntasks-per-node=4 ./ior \
        -a POSIX \
        -F \
        -C \
        -e \
        -g \
        -b 4m \
        -t 4m \
        -s 1638 \
        -o $SCRATCH/IOR_file \
        -v \

where

- `-a POSIX` uses the POSIX API (this is the default option)
- `-F` uses file-per-process I/O (not default)
- `-C` reorders tasks between read and write to ensure nodes read data written
   by their neighbors
- `-e` uses fsync after write to ensure the timing reflects the full write time
- `-g` uses `MPI_Barrier()` after the write and read phases
- `-b 4m` uses a 4 MiB block size
- `-t 4m` uses 4 MiB transactions
- `-s 1638` uses 1,638 segments and was chosen so that the job writes out 24 TiB
- `-o $SCRATCH/IOR_file` writes to the `$SCRATCH` file system
- `-v` increases verbosity a little

### HACC-IO

Both read and write versions of the HACC-IO benchmark take two command-line
arguments:

- the number of particles, and
- the path to an output file

For example,

    srun -n 6144 ./hacc_io_write 20971520 $SCRATCH/hacc.dat

Each particle is 38 bytes, so the above example will generate a collection of
output files (one file per process by default), each containing 760 MiB of
particle data and a small header.

When running the `hacc_io_read` benchmark, specify the same output file path
(e.g., `$SCRATCH/hacc.dat`) used when running `hacc_io_write`.  HACC-IO will
use this as the base file name and suffix per-rank identifiers on each file.

### VPIC-IO

VPIC-IO is an I/O kernel that emulates the writing of a checkpoint file as
performed by the [VPIC application][].  It is characterized by 

- single shared-file writes using the HDF5 library and MPI-IO
- 1 GiB of data per MPI process
- 128 MiB transactions

The VPIC-IO benchmark was written by [Suren Byna] and depends on the
[H5Part library][].  The H5Part library included in this repository is derived
from [H5Part 1.6.6][] and requires HDF5 1.8.

The `vpicio_uni` application takes a single command-line argument, which is the
path to the output file.  For example,

    srun -n 32 ./vpicio_uni $SCRATCH/vpicio.h5part

will run VPIC-IO across 32 processes and write 32 GiB (a fixed 1 GiB/process) to
`$SCRATCH/vpicio.h5part` (which need not exist).  It does not touch any other
file systems or `$PWD`.

### BD-CATS-IO

BD-CATS-IO is an I/O trace written by Suren that is derived from the
[BD-CATS clustering system][] which performs clustering analysis on large H5Part
files generated by VPIC (or VPIC-IO).

BD-CATS-IO requires you to specify at least two command line arguments when
running it:

- `-f /path/to/vpic.h5part` - the H5Part file to be read
- `-d "/ReadGroup/ReadDataset"` - the dataset in the given H5Part file to scan.
   Multiple `-d` arguments can/should be passed.

For example, the VPIC-IO kernel generates six datasets which can be scanned
using BD-CATS-IO by doing

    srun -n 32 ./dbscan_read -f $SCRATCH/vpicio.h5part -d '/Step#0/x' \
                                                       -d '/Step#0/y' \
                                                       -d '/Step#0/z' \
                                                       -d '/Step#0/px' \
                                                       -d '/Step#0/py' \
                                                       -d '/Step#0/pz'

[VPIC application]: https://github.com/losalamos/vpic
[Suren Byna]: https://sdm.lbl.gov/~sbyna/
[H5Part library]: http://vis.lbl.gov/Research/H5Part/
[H5Part 1.6.6]: https://codeforge.lbl.gov/projects/h5part/
[BD-CATS clustering system]: http://dx.doi.org/10.1145/2807591.2807616
